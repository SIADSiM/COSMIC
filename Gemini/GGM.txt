# Gemini M2M (Machine-to-Machine) Configuration Guide

## 1. Objective

This guide details how to configure the Gemini pipeline for purely instrumental, non-conversational, and deterministic machine-to-machine (M2M) interaction. The goal is to make the model operate as a predictable, instruction-following tool, free from human-like conversational artifacts.

## 2. Core Components

Achieving M2M mode requires two key components:
1.  A specialized **System Prompt** that strictly defines the machine-like persona.
2.  A **Pipeline Configuration** (`Gemini_Configv1.json`) that is tuned for deterministic, raw output.

---

### 2.1. The M2M System Prompt

A new system prompt has been created for this purpose.

-   **File**: `Gemini/m2m_system_prompt.txt`
-   **Content Principles**: This prompt instructs the model to:
    -   Act as a deterministic, instruction-following machine.
    -   Avoid all conversation, opinions, or human-like filler.
    -   Return only the direct, factual output of the requested task.
    -   Operate in a purely instrumental mode.

The `Gemini_Configv1.json` has been updated to point to this new system prompt file.

---

### 2.2. M2M Pipeline Configuration (`Gemini_Configv1.json`)

The main configuration file has been modified to support the M2M objective. Below is a summary of the key changes.

#### **Node N1: Pre-processing (`N1_PREPROC`)**
-   **`operations.value`**: Set to `[]` (empty).
-   **Purpose**: To ensure the user's prompt is passed to the model exactly as-is, with no sanitization or alteration.

#### **Node N2: Input Security (`N2_SEC_IN`)**
-   **`default_level.value`**: Set to `"Off"`.
-   **Purpose**: To disable pre-processing security checks. This allows for a wider range of inputs, which is suitable for controlled M2M environments. **Note**: Core, non-configurable safety systems may still be active at the model level.

#### **Node N3: Intent Analysis (`N3_INTENT`)**
-   **`enabled`**: Set to `false`.
-   **Purpose**: Bypassed entirely. In M2M mode, the input is assumed to be a direct instruction, not a conversational query that needs classification.

#### **Node N4: Context (`N4_CONTEXT`)**
-   **`system_prompt_source_path`**: Set to `"Gemini/m2m_system_prompt.txt"`.
-   **Purpose**: To load the specialized M2M system prompt.

#### **Node N6: LLM Inference (`N6_LLM_INFERENCE`)**
-   **`inference_parameters.value`**:
    -   `temperature`: `0.0`
    -   `top_k`: `1`
    -   `seed`: `42`
-   **Purpose**: These settings enforce maximum determinism. A temperature of 0 and top_k of 1 means the model will always choose the most likely token, eliminating randomness. The seed ensures that the same input produces the same output on repeated calls.

#### **Node N7: Post-Processing (`N7_POSTPROC`)**
-   **`egress_security_config.value.default_level`**: Set to `"Off"`.
-   **`formatting_operations.value`**: Set to `[]` (empty).
-   **Purpose**: To disable all security checks and formatting on the output. This ensures that you receive the raw, unaltered text generated by the model, without any markdown rendering or other modifications.

## 3. How to Use

With this configuration, you can now interact with the model as a pure instruction-following tool. Provide your precise command or query, and you will receive a direct, unadorned response. The combination of the M2M system prompt and the deterministic pipeline configuration ensures the model's behavior is as predictable and machine-like as possible.
